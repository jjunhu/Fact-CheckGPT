
# Retrieval-Then-Entailment Pipeline for Claim Verification

## Table of Contents
1. [Project Overview](#project-overview)
2. [Pipeline Description](#pipeline-description)
3. [Dataset Information](#dataset-information)
4. [Installation](#installation)
5. [Usage](#usage)
6. [Results](#results)
7. [Visualization](#visualization)
8. [Contributing](#contributing)
9. [License](#license)

## Project Overview

This project implements a retrieval-then-entailment pipeline designed to verify claims based on retrieved text passages. The pipeline is evaluated using multiple retrieval models, including DPR, Contriever, BM25, and a fine-tuned version of Contriever. The main goal is to assess the effectiveness of these models in accurately retrieving relevant passages and determining whether these passages support or refute the given claims.

## Pipeline Description

The pipeline operates in two main stages:

1. **Retrieval**: The first stage involves retrieving the most relevant text passages from a large corpus based on the input claim. Various retrieval models are used and compared, including neural models like DPR and traditional models like BM25.

2. **Entailment**: In the second stage, the retrieved passages are passed into an entailment model to determine if they support, contradict, or are neutral to the claim. The entailment model used in this project is based on BART, which is fine-tuned on the MNLI dataset.

### Execution Process

The pipeline is executed using the following code:
\`\`\`python
result = self.classifier(f"{truncated_premise}{self.tokenizer.sep_token}{claim}")
\`\`\`
Here, \`truncated_premise\` represents the retrieved document chunk, and \`claim\` is the input claim to be verified.

## Dataset Information

### Dataset Preparation

The datasets used in this project consist of both positive and negative claims:

- **Positive Data**: Taken from depth-3 retrieval data, where each summary sentence is used as a claim, and the entailment result is \`Entailment\`.

- **Negative Data**: Generated by passing the document through another BART model that produces a negative version of the document. The entailment result for these claims is either \`Contradiction\` or \`Neutral\`.

### Testing Dataset

The testing dataset consists of both correct and incorrect claims to evaluate whether the pipeline can distinguish between them.

## Installation

### Prerequisites

Ensure you have Python 3.7+ installed. Additionally, the following Python packages are required:

- \`transformers\`
- \`torch\`
- \`tqdm\`
- \`pandas\`
- \`matplotlib\`

### Installation Steps

1. Clone this repository:
   \`\`\`bash
   git clone https://github.com/yourusername/retrieval-entailment-pipeline.git
   cd retrieval-entailment-pipeline
   \`\`\`

2. Install the required packages:
   \`\`\`bash
   pip install -r requirements.txt
   \`\`\`

3. Download the necessary models and datasets:
   - BART fine-tuned on MNLI: [BART-MNLI](https://huggingface.co/facebook/bart-large-mnli)
   - Retrieval models: [DPR](https://huggingface.co/facebook/dpr), [Contriever](https://huggingface.co/facebook/contriever), [BM25](#)

## Usage

### Running the Pipeline

1. **Prepare the Dataset**:
   - Ensure your dataset is properly formatted and located in the specified directory.

2. **Run the Retrieval-Then-Entailment Pipeline**:
   \`\`\`bash
   python run_pipeline.py --model_name dpr --dataset_path ./data/test_data.json
   \`\`\`

3. **Analyze Results**:
   - The \`analyze_results\` function processes the output to compute various accuracy metrics and entailment counts.

### Configuration

You can modify the configuration parameters such as model type, batch size, learning rate, and more by editing the \`config.json\` file.

## Results

The pipeline's performance is evaluated using several metrics, including overall accuracy, true accuracy, and the percentage of time the target passage is in the top 10 retrieved passages.

- **Overall Accuracy**: Measures how often the retrieval model's prediction matches the correct entailment.
- **True Accuracy**: Reflects the model's accuracy when the gold passage is retrieved and leads to the correct entailment.
- **Percentage in Top 10**: Indicates the frequency with which the target passage is found within the top 10 retrieved passages.

The results are saved in the \`results/\` directory, and key metrics are displayed on the console.

## Contributing

We welcome contributions to improve this project. Please follow these steps:

1. Fork the repository.
2. Create a new branch (\`git checkout -b feature-branch\`).
3. Commit your changes (\`git commit -am 'Add new feature'\`).
4. Push to the branch (\`git push origin feature-branch\`).
5. Create a new Pull Request.

## License

This project is licensed under the MIT License. See the \`LICENSE\` file for more details.
