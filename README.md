
# **✨ Fact-CheckGPT: [A *Dual-Phase* Approach with Enhanced Retrieval and Entailment Models ✨**](https://jjunhu.github.io/Fact-CheckGPT/Retrieval_Entailment_Final_Report.pdf)

## Table of Contents
1. [Project Overview](#project-overview)
2. [Pipeline Description](#pipeline-description)
3. [Dataset Information](#dataset-information)
4. [Installation](#installation)
5. [Usage](#usage)
6. [Results](#results)
7. [Visualization](#visualization)
8. [Contributing](#contributing)
9. [License](#license)

## Project Overview

This project aims to mitigate and resolve the issue of inadequate context length in modern large language models, in particular, models tailored for fact-checking. The current frontier approach consists of passing the entire context -– entire book, entire news database, into a language model, that frankly often either exceeds the context window lenghth or incurs too much cost. Our approach aims to drastically decrease that cost by reducing the length of each prompt via extracting high-quality, tailored, and claim-specific context that is only a thousandth of the original context length. We implement a retrieval-then-entailment pipeline designed to verify claims based on retrieved text passages. The pipeline is evaluated using multiple retrieval models, including DPR, Contriever, BM25, and a fine-tuned version of Contriever. The main goal is to assess the effectiveness of these models in accurately retrieving relevant passages and determining whether these passages support or refute the given claims.

## Pipeline Description

The pipeline operates in two main stages:

1. **Retrieval**: The first stage involves retrieving the most relevant text passages from a large corpus based on the input claim. Various retrieval models are used and compared, including neural models like DPR and traditional models like BM25.

2. **Entailment**: In the second stage, the retrieved passages are passed into an entailment model to determine if they support, contradict, or are neutral to the claim. The entailment model used in this project is based on BART, which is fine-tuned on the MNLI dataset.

### Execution Process

The pipeline is executed using the following code:
```python
result = self.classifier(f"{truncated_premise}{self.tokenizer.sep_token}{claim}")
```
Here, \`truncated_premise\` represents the retrieved document chunk, and \`claim\` is the input claim to be verified.

## Dataset Information

### Dataset Preparation

The datasets used in this project consist of both positive and negative claims:

- **Positive Data**: Taken from depth-3 retrieval data, where each summary sentence is used as a claim, and the entailment result is \`Entailment\`.

- **Negative Data**: Generated by passing the document through another BART model that produces a negative version of the document. The entailment result for these claims is either \`Contradiction\` or \`Neutral\`.

### Testing Dataset

The testing dataset consists of both correct and incorrect claims to evaluate whether the pipeline can distinguish between them.

## Installation

### Prerequisites

Ensure you have Python 3.7+ installed. Additionally, the following Python packages are required:

- \`transformers\`
- \`torch\`
- \`tqdm\`
- \`pandas\`
- \`matplotlib\`

### Installation Steps

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/retrieval-entailment-pipeline.git
   cd retrieval-entailment-pipeline
   ```

2. Install the required packages:
   ```bash
   pip install -r requirements.txt
   ```

3. Download the necessary models and datasets:
   - BART fine-tuned on MNLI: [BART-MNLI](https://huggingface.co/facebook/bart-large-mnli)
   - Retrieval models: [DPR](https://huggingface.co/facebook/dpr), [Contriever](https://huggingface.co/facebook/contriever), [BM25](#)

## Usage

### Running the Pipeline and Fine-Tuning

1. **Prepare the Dataset**:
   - Ensure your dataset is properly formatted and located in the specified directory.

2. **Run the Retrieval-Then-Entailment Pipeline**:
   ```bash
   python run_entailment_pipeline.py --entailment_model_name --retriever_model_name 
   ```

3. **Fine-Tuning the Retriever Model**:
   ```bash
   python fine_tune.py --batch_size 
   ```

4. **Other Retrievers**:
   ```bash
   python run_retrievers.py --retriever_model_name 
   ```


### Slurm 

You can make sure of the slurm scripts to schedule and manage running tasks in the \slurm_scripts directory. 


## Contributing

We welcome contributions to improve this project. Please follow these steps:

1. Fork the repository.
2. Create a new branch (\`git checkout -b feature-branch\`).
3. Commit your changes (\`git commit -am 'Add new feature'\`).
4. Push to the branch (\`git push origin feature-branch\`).
5. Create a new Pull Request.

## License

This project is licensed under the MIT License. See the \`LICENSE\` file for more details.
